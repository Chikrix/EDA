---
title: "K-means Clustering"
author: "Chidi Justice"
date: "8/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is another clustering algorithm in high dimensional data analysis. Like many clustering algorithms, it can't be summarised by a single formula, and is iterative. The basic idea is that we're trying to find the centroids of a fixed number of clusters of points in a high dimensional space. The k-means approach is that data are partitioned into groups at each iteration of the algorithm. We need to pre-specify the number of clusters we expect, although this isn't usually known, we can take a guess of this and can always change it each time. The way the algorithm works is;  
* Select with a fixed number as the number of clusters greater than or equal to 2.  
* Randomly start with number of points that'll be used as the initial starting centroids  
* Assign points to their closest centroid  
* Recalculate centroid positions and repeat  

K-means clustering requires a distance metric, a fixed number of clusters, and an initial guess to the cluster centroids as there's no approach to determining the number of starting centroids.  K-means clustering prodices a final estimate of cluster centroids, and an assignment of each point to its respective cluster.  What actually moves is our centroid, at each iteration, the position of our centroid is recalculated and points are assigned to their closest centroid.  We keep track of the distance that each centroid moves between iterations, when this distance is relatively small, we stop the algoroithm (we usually don't do this manually)  

In R, we use the `kmeans` function to implement k-means clustering. The key parameters we need to specify are `x` and `centers`, which respectively represent a matrix/dataframe and an integer indicating the number of clusters or a matrix indicating the locations of the initial cluster centroids. 
```{r}
set.seed(1234)
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)

plot(x, y, col = "blue", pch = 19, cex = 1.5)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

df <- data.frame(x, y)
kmeans_obj <- kmeans(df, centers = 3)
names(kmeans_obj)

#### Cluster list
kmeans_obj$cluster

df$clusters = kmeans_obj$cluster
with(df, plot(x, y, col = clusters, pch = 19))
```

As we can see from the above, with even the small data, kmeans was able to cluster points appropriately. We can build heatmaps or image plots from k-means solutions;  
```{r}
set.seed(1234)
datamatrix <- as.matrix(df)[sample(1:12),][,-c(3)]
kmeans_obj <- kmeans(datamatrix, centers = 3)

par(mfrow = c(1, 2))
image(t(datamatrix)[, nrow(datamatrix):1], yaxt = "n", main = "Original data")
image(t(datamatrix)[, order(kmeans_obj$cluster)], yaxt = "n", main = "Clustered data")

```

The idea above is that each cell of the plot above is colored in a manner proportional to the value corresponding matrix element. This can be very useful, especially for high-dimensional datasets that can be visualised. The plot above is ordered in a way such that all the rows in a given cluster are grouped together so their proportion is well seen in the dataset. 





















